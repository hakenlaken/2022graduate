# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12_l_MkQA71r34PCAlc2MYGrehNIRk2cA
"""

import pandas as pd
df = pd.read_csv (r'/content/dataset_sdn.csv')
print (df)

df.describe()

group_one = df.groupby(['dt','switch','src','dst','Protocol', 'label'])['pktcount','bytecount','tx_bytes','rx_bytes','tx_kbps','rx_kbps','tot_kbps'].sum().reset_index()
group_label = df.groupby(['dt','switch','src','dst','Protocol'])['label'].max().reset_index()
#df[['tx_bytes_sum', 'rx_bytes_sum']]=df.groupby(['dt','switch','src','dst'])['tx_bytes','rx_bytes'].transform('sum')
#df.groupby(['dt','switch','src','dst']).agg({'label': 'mean', ['tx_bytes','rx_bytes','tx_kbps','rx_kbps','tot_kbps']: 'sum'})

group_one = group_one.sort_values(by=['dt']).reset_index()
group_one['dt'] = group_one['dt'] - 2488

group_label = group_label.sort_values(by=['dt']).reset_index()
group_label['dt'] = group_label['dt'] - 2488

group_one.head(40)

group_all = group_one.groupby(['dt'])['pktcount','bytecount','tx_bytes','rx_bytes','label'].sum().reset_index()
#group_all['label'].plot()
group_all.describe()

for idx, row in group_all.iterrows():
    if  group_all.loc[idx,'label'] == 31 or group_all.loc[idx,'label'] >= 35 or group_all.loc[idx,'pktcount'] >= 5.3e+07 or group_all.loc[idx,'bytecount'] >= 5.5e+10 or group_all.loc[idx,'tx_bytes'] >= 8.5e+10 or group_all.loc[idx,'rx_bytes'] >= 8.5e+10 or group_all.loc[idx,'dt'] == 12097 or group_all.loc[idx,'dt'] == 29115 or group_all.loc[idx,'dt'] == 39697 or group_all.loc[idx,'dt'] == 4004 :
        group_all.loc[idx,'label'] = 1
    else:
        group_all.loc[idx,'label'] = 0
# group_all['label'] = 1 if (group_all['label'] >= 35) else 0
group_all['label'].value_counts()

group_all.describe()

"""Train data"""

import plotly.graph_objects as go

#s1_group = group_one[group_one['switch'] == 1]
#s1_group = s1_group.groupby(['dt','switch','Protocol','label'])['bytecount'].sum().reset_index()
#s1_group.head(10)

layout = dict(xaxis=dict(title='dt'), yaxis=dict(title='bytecount'), title="Перегрузки в наборе данных")
fig = go.Figure(layout=layout)
df_non_anomaly = group_all[group_all['label'] == 0]
df_anomaly = group_all[group_all['label'] == 1]
fig.add_trace(go.Scatter(x=df_non_anomaly['dt'], y=df_non_anomaly['bytecount'], 
                         mode='markers', name='Нет перегрузки',
                         marker=dict(color='blue')))
fig.add_trace(go.Scatter(x=df_anomaly['dt'], y=df_anomaly['bytecount'], 
                         mode='markers', name='Есть перегрузка',
                         marker=dict(color='green', size=10)))

"""Train and Validate data"""

train = group_all.iloc[:266, :]
valid = group_all.iloc[266:, :]
train.describe()
valid.describe()

"""# Dataset preparation


"""

import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np

class CPUDataset(Dataset):
    def __init__(self, data: pd.DataFrame, size: int):
        self.chunks = torch.FloatTensor(data['stand_value']).unfold(0, size, size)
        
    def __len__(self):
        return self.chunks.size(0)
    
    def __getitem__(self, i):
        x = self.chunks[i]
        return x

#train_values = train['stand_value'].values.astype(np.float32).flatten()
#valid_values = valid['stand_value'].values.astype(np.float32).flatten()



"""# ARIMA model"""

import statsmodels.api as sm
from itertools import product

def write_predict(train_df: pd.DataFrame, valid_df: pd.DataFrame):
    
    Qs = range(0, 2)
    qs = range(0, 3)
    Ps = range(0, 3)
    ps = range(0, 3)
    D=1
    d=1
    parameters = product(ps, qs, Ps, Qs)
    parameters_list = list(parameters)
    
    
    results = []
    best_aic = float("inf")
    for param in parameters_list:
        try:
            model=sm.tsa.statespace.SARIMAX(
                train_df.bytecount, order=(param[0], d, param[1]),
                seasonal_order=(param[2], D, param[3], 12),
                initialization='approximate_diffuse'
                ).fit()
        except ValueError:
            print('wrong parameters:', param)
            continue
        aic = model.aic
        if aic < best_aic:
            best_model = model
            best_aic = aic
            best_param = param
        results.append([param, model.aic])
    
    
    train_df['predict'] = best_model.predict()
    train_df['predict'].fillna(0, inplace=True)
    
    
    best_model_valid = sm.tsa.statespace.SARIMAX(
        valid_df.value, order=(best_param[0], d, best_param[1]),
        seasonal_order=(best_param[2], D, best_param[3], 12),
        initialization='approximate_diffuse'
        ).fit()
    valid_df['predict'] = best_model_valid.predict()
    valid_df['predict'].fillna(0, inplace=True)
    


write_predict(train, valid)





"""# CNN model

"""

import torch 
from torch.utils.data import Dataset, DataLoader

class CPUDataset(Dataset):
    def __init__(self, data: pd.DataFrame, size: int, 
                 step: int = 1):
        self.chunks = torch.FloatTensor(data['stand_value']).unfold(0, size+1, step)
        self.chunks = self.chunks.view(-1, 1, size+1)
    def __len__(self):
        return self.chunks.size(0)
    
    def __getitem__(self, i):
        x = self.chunks[i, :, :-1]
        y = self.chunks[i, :, -1:].squeeze(1)
        return x, y

n_factors = 10
train_ds = CPUDataset(train, n_factors)
valid_ds = CPUDataset(valid, n_factors)

import torch.nn as nn

def conv_layer(in_feat, out_feat, kernel_size=3, stride=1,
               padding=1, relu=True):
    res = [
        nn.Conv1d(in_feat, out_feat, kernel_size=kernel_size,
                  stride=stride, padding=padding, bias=False),
        nn.BatchNorm1d(out_feat),
    ]
    if relu:
        res.append(nn.ReLU())
    return nn.Sequential(*res)

class ResBlock(nn.Module):
    def __init__(self, in_feat, out_feat):
        super().__init__()
        self.in_feat, self.out_feat = in_feat, out_feat
        self.conv1 = conv_layer(in_feat, out_feat)
        self.conv2 = conv_layer(out_feat, out_feat, relu=False)
        if self.apply_shortcut:
            self.shortcut = conv_layer(in_feat, out_feat,
                                       kernel_size=1, padding=0,
                                       relu=False)
    
    def forward(self, x):
        out = self.conv1(x)
        if self.apply_shortcut:
            x = self.shortcut(x)
        return x + self.conv2(out)
    
    @property
    def apply_shortcut(self):
        return self.in_feat != self.out_feat

class AdaptiveConcatPool1d(nn.Module):
    def __init__(self):
        super().__init__()
        self.ap = nn.AdaptiveAvgPool1d(1)
        self.mp = nn.AdaptiveMaxPool1d(1)
    
    def forward(self, x): 
        return torch.cat([self.mp(x), self.ap(x)], 1)

class CNN(nn.Module):
    def __init__(self, out_size):
        super().__init__()
        self.base = nn.Sequential(
            ResBlock(1, 8), #shape = batch, 8, n_factors
            ResBlock(8, 8), 
            ResBlock(8, 16), #shape = batch, 16, n_factors
            ResBlock(16, 16),
            ResBlock(16, 32), #shape = batch, 32, n_factors
            ResBlock(32, 32),
            ResBlock(32, 64), #shape = batch, 64, n_factors
            ResBlock(64, 64),
        )
        self.head = nn.Sequential(
            AdaptiveConcatPool1d(), #shape = batch, 128, 1
            nn.Flatten(),
            nn.Linear(128, out_size)
        )
        
    def forward(self, x):
        out = self.base(x)
        out = self.head(out)
        return out

def train_model(model: CNN, dataloaders: dict, optimizer: opt.Optimizer, 
                scheduler, criterion, device: torch.device, epochs: int):
    losses_data = {'train': [], 'valid': []}
    model.to(device)
    
    # Loop over epochs
    for epoch in tqdm(range(epochs)):
        print(f'Epoch {epoch}/{epochs-1}')
        
        # Training and validation phases
        for phase in ['train', 'valid']:
            if phase == 'train':
                model.train()
            else:
                model.eval()

            running_loss = 0.
            running_total = 0.
            
            # Loop over batches of data
            for idx, batch in tqdm(enumerate(dataloaders[phase]), 
                                   total=len(dataloaders[phase]), 
                                   leave=False
                                   ):
                x, y = batch
                x = x.to(device)
                y = y.to(device)

                optimizer.zero_grad()

                with torch.set_grad_enabled(phase == 'train'):
                    out = model(x)
                    loss = criterion(out, y)

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()
                        scheduler.step()

                running_loss += loss.item() * y.size(0)
                running_total += y.size(0)

            epoch_loss = running_loss / running_total
            print(f'{phase.capitalize()} Loss: {epoch_loss}')
            losses_data[phase].append(epoch_loss)
    return losses_data

epochs = 50
cnn_model = CNN(out_size=1)
dataloaders = {
    'train': DataLoader(train_ds, batch_size=128, shuffle=True),
    'valid': DataLoader(valid_ds, batch_size=128)
}
optim = opt.Adam(cnn_model.parameters(), lr=1e-1, weight_decay=1e-3)
sched = opt.lr_scheduler.OneCycleLR(optim, max_lr=1e-3, steps_per_epoch=len(dataloaders['train']), epochs=epochs)
criterion = nn.MSELoss()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

losses = train_model(cnn_model, dataloaders, optim, sched, criterion, device, epochs)
layout = dict(xaxis=dict(title='Epoch'), yaxis=dict(title='Loss'))
fig = go.Figure(layout=layout)
fig.add_trace(go.Scatter(y=losses['train'], mode='lines', name='Train Loss',))
fig.add_trace(go.Scatter(y=losses['valid'], mode='lines', name='Valid Loss'))

cnn_model = cnn_model.eval()

with torch.no_grad():
    res_train = cnn_model(train_ds[:][0].to(device))
res_train = res_train.cpu()

with torch.no_grad():
    res_valid = cnn_model(valid_ds[:][0].to(device))
res_valid = res_valid.cpu()

"""# LSTM model"""

class CPUDataset(Dataset):
    def __init__(self, data: pd.DataFrame, size: int):
        self.chunks = torch.FloatTensor(data['stand_value']).unfold(0, size, size)
        
    def __len__(self):
        return self.chunks.size(0)
    
    def __getitem__(self, i):
        x = self.chunks[i]
        return x

train_ds = CPUDataset(train, 64)
valid_ds = CPUDataset(valid, 64)

class LSTMModel(nn.Module):
    def __init__(self, in_size, hidden_size, out_size, device):
        super().__init__()
        self.hidden_size = hidden_size
        self.lstm = nn.LSTM(in_size, hidden_size)
        self.linear = nn.Linear(hidden_size, out_size)
        self.device = device
        self.init_hidden()
        
    def forward(self, x):
        out, self.hidden_state = self.lstm(
            x.view(len(x), 1, -1), self.hidden_state
        )
        self.hidden_state = tuple(
            [h.detach() for h in self.hidden_state]
        )
        out = out.view(len(x), -1)
        out = self.linear(out)
        return out
    
    def init_hidden(self):
        self.hidden_state = (
            torch.zeros((1, 1, self.hidden_size)).to(self.device),
            torch.zeros((1, 1, self.hidden_size)).to(self.device))

def train_model(model: LSTMModel, dataloaders: dict, optimizer: opt.Optimizer, 
                scheduler, criterion, device: torch.device, epochs: int):
    losses_data = {'train': [], 'valid': []}
    model.to(device)
    for epoch in tqdm(range(epochs)):
        print(f'Epoch {epoch}/{epochs-1}')
        for phase in ['train', 'valid']:
            if phase == 'train':
                model.train()
            else:
                model.eval()

            running_loss = 0.
            running_total = 0.
            
        
            for idx, sequence in enumerate(dataloaders[phase]):
                value = sequence
                value = value.to(device)

                optimizer.zero_grad()

                with torch.set_grad_enabled(phase == 'train'):
                    out = model(value.view(-1, 1))
                    loss = criterion(out.view(-1), value.view(-1))
        

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()
                        scheduler.step()

                running_loss += loss.item() * out.size(0)
                running_total += out.size(0)

            epoch_loss = running_loss / running_total
            print(f'{phase.capitalize()} Loss: {epoch_loss}')
            losses_data[phase].append(epoch_loss)
    return losses_data

epochs = 50
model = LSTMModel(1, 128, 1, device)
dataloaders = {
    'train': DataLoader(train_ds, batch_size=1),
    'valid': DataLoader(valid_ds, batch_size=1)
}
optim = opt.Adam(params=model.parameters(), lr=1e-3)
sched = opt.lr_scheduler.OneCycleLR(
  optimizer, max_lr=1e-3, steps_per_epoch=len(dataloaders['train']), epochs=total_epoch_count
)
criterion = nn.MSELoss()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

losses = train_model(model, dataloaders, optim, sched, criterion, device, epochs)

train_values = train['stand_value'].values.astype(np.float32).flatten()
valid_values = valid['stand_value'].values.astype(np.float32).flatten()

model.eval()

with torch.no_grad():
    res_train = model(torch.tensor(train_values).to(device))
res_train = res_train.cpu()

with torch.no_grad():
    res_valid = model(torch.tensor(valid_values).to(device))
res_valid = res_valid.cpu()
NtFlg = 0

"""# ERRORS calculation

ARIMA error
"""

def calculate_prediction_errors(input_data):
    return (abs(input_data['value'] -input_data['predict'])).to_numpy()

train_pred_errors = calculate_prediction_errors(train)
valid_pred_errors = calculate_prediction_errors(valid)

"""CNN’s errors"""

def calculate_prediction_errors(
    model: CNN, dataset: CPUDataset, criterion, 
    device: torch.device
    ):
    with torch.no_grad():
        errors = []
        for x, y in tqdm(dataset):
            x = x.to(device)[None]
            y = y.to(device)[None]
            predicted = model(x)
            prediction_error = criterion(predicted, y)
            errors.append(prediction_error.cpu())
        return errors

train_pred_errors = calculate_prediction_errors(cnn_model, train_ds, criterion, device)
valid_pred_errors = calculate_prediction_errors(cnn_model, valid_ds, criterion, device)

"""LSTM’s errors"""

def calculate_prediction_errors(target, predicted, criterion):
    reconstruction_errors = []
    for t, p in zip(target, predicted):
        reconstruction_errors = np.append(
            reconstruction_errors, 
            criterion(p, t).cpu().numpy().flatten()
        )
    return reconstruction_errors

train_pred_errors = calculate_prediction_errors(
    res_train.view(-1), torch.tensor(train_values).view(-1), criterion
)
valid_pred_errors = calculate_prediction_errors(
    res_valid.view(-1), torch.tensor(valid_values).view(-1), criterion
)

"""# Threshold calculation

Static threshold
"""

pred_error_threshold = np.mean(train_pred_errors) + 3 * np.std(train_pred_errors)

"""Dynamic threshold"""

train_pred_errors_windowed = pd.Series(train_pred_errors).rolling(window=window, min_periods=1)
train_dynamic_threshold = train_pred_errors_windowed.mean() + std_coef * train_pred_errors_windowed.std()

valid_pred_errors_windowed = pd.Series(valid_pred_errors).rolling(window=window, min_periods=1)
valid_dynamic_threshold = valid_pred_errors_windowed.mean() + std_coef * valid_pred_errors_windowed.std()

"""# Metric calculation"""

from sklearn.metrics import precision_recall_fscore_support

def calculate_metrics(
    ground_truth: pd.DataFrame, anomalies_idxs: list
    ):
    predictions = pd.DataFrame(
        index=range(len(ground_truth)), 
        columns=['anomaly_predicted']
    )
    predictions['anomaly_predicted'] = 0
    predictions.iloc[anomalies_idxs] = 1
    
    confusion_matrix = pd.crosstab(
        ground_truth.loc[:, 'anomaly_label'],
        predictions['anomaly_predicted'], 
        margins=True
    )
    precision, recall, f1, _ = precision_recall_fscore_support(
        ground_truth.loc[:, 'anomaly_label'],
        predictions['anomaly_predicted'], 
        beta=2., 
        average='binary'
    )
    return confusion_matrix, precision, recall, f1

"""ARIMA results with static threshold"""

def detect_anomalies(pred_error_threshold,df):
    test_reconstruction_errors = calculate_reconstruction_errors(df)
    predicted_anomalies = list(
        map(lambda v: 1 if v > pred_error_threshold else 0,
        test_reconstruction_errors)
    )
    df['anomaly_predicted'] = predicted_anomalies
    indexes = [i for i, x in enumerate(predicted_anomalies) if x == 1]
    return indexes

train_anomalies_idxs = detect_anomalies(
    pred_error_threshold, training_data_frame
)
valid_anomalies_idxs = detect_anomalies(
    pred_error_threshold, valid_data_frame
)

train_conf_matrix, *train_metrics = calculate_metrics(
    training_data_frame, train_anomalies_idxs
)

print(f'Train:\n Precision: {train_metrics[0]:.3f}\n' 
      f'Recall: {train_metrics[1]:.3f}\n' 
      f'F2 score: {train_metrics[2]:.3f}')

valid_conf_matrix, *valid_metrics = calculate_metrics(
    valid_data_frame, valid_anomalies_idxs
)

print(f'Valid:\n Precision: {valid_metrics[0]:.3f}\n' 
      f'Recall: {valid_metrics[1]:.3f}\n' 
      f'F2 score: {valid_metrics[2]:.3f}')

"""ARImA results with dynamic threshold"""

def detect_anomalies(df, errors, pred_error_thresholds):
    df['error'] = errors
    df['upper_bound'] = pred_error_thresholds
    indices = df.index[df['error'] >= df['upper_bound']].values.tolist()
    indices = [i for i in indices]
    return indices

train_anomalies_idxs = detect_anomalies(
    training_data_frame, train_pred_errors, train_dynamic_threshold
)
valid_anomalies_idxs = detect_anomalies(
    valid_data_frame, valid_pred_errors, valid_dynamic_threshold
)

"""NN’s static results"""

from typing import Union

def detect_anomalies(
    result: torch.Tensor, dataset: CPUDataset, 
    threshold: Union[float, pd.Series], n_factors: int = 0
):
    anomalies_idxs = []
    for i in range(len(dataset)):
        if type(threshold) == pd.Series:
            is_anomaly = (criterion(result[i], dataset[i][1]) > threshold[i])
        else:
            is_anomaly = (criterion(result[i], dataset[i][1]) > threshold)
        if is_anomaly:
            anomalies_idxs.append(i + n_factors)
     return anomalies_idxs

"""CNN with static threshold"""

train_anomalies_idxs = detect_anomalies(
    res_train, train_ds, pred_error_threshold, n_factors
)
valid_anomalies_idxs = detect_anomalies(
    res_valid, valid_ds, pred_error_threshold, n_factors
)

train_conf_matrix, *train_metrics = calculate_metrics(
    train, train_anomalies_idxs
)

valid_conf_matrix, *valid_metrics = calculate_metrics(
    valid, valid_anomalies_idxs
)

"""CNN with dynamic threshold"""

train_anomalies_idxs = detect_anomalies(
    res_train, train_ds, train_dynamic_threshold, n_factors
)
valid_anomalies_idxs = detect_anomalies(
    res_valid, valid_ds, valid_dynamic_threshold, n_factors
)

"""LSTM with static threshold"""

train_anomalies_idxs = detect_anomalies(
    res_train, torch.tensor(train_values), pred_error_threshold
)
valid_anomalies_idxs = detect_anomalies(
    res_valid, torch.tensor(valid_values), pred_error_threshold
)
"""LSTM with dynamic threshold"""
train_anomalies_idxs = detect_anomalies(
    res_train, torch.tensor(train_values), train_dynamic_threshold
)
valid_anomalies_idxs = detect_anomalies(
    res_valid, torch.tensor(valid_values), valid_dynamic_threshold
)

print(f'Overload?\n Answer: {NtFlg}\n')

"""# RESULTS

----------
статический порог
----------

Модель
Precision (Точность, ошибки первого рода) | Recall (Полнота, ошибки второго рода) | F2-score
LSTM:
0.333 / 0.002 | 0.5 / 1.0 | 0.454 / 0.01

CNN:
0.5 / 0.002 | 0.5 / 1.0 | 0.5 / 0.01

ARIMA:
0.044 / 0.019 | 1.0 / 1.0 | 0.187 / 0.088

----------
динамический порог
----------

Модель
Precision (Точность, ошибки первого рода) | Recall (Полнота, ошибки второго рода) | F2-score
LSTM:
0.667 / 1.0 | 1.0 / 1.0 | 0.909 / 1.0

CNN:
0.333 / 0.25 | 0.5 / 1.0 | 0.454 / 0.625

ARIMA:
0.333 / 0.667 | 0.5 / 1.0 | 0.454 / 0.909
"""